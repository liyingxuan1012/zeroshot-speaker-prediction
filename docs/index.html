<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zero-Shot Speaker Prediction in Comics</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion</h1>
          <h2 class="title is-size-3 publication-title">ACMMM 2024 (Oral)</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=n_dyrz0AAAAJ&hl=en">Yingxuan Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=Pr-m0cUAAAAJ&hl=en">Ryota Hinami</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.hal.t.u-tokyo.ac.jp/~aizawa">Kiyoharu Aizawa</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://yusukematsui.me/index.html">Yusuke Matsui</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Tokyo,</span>
            <span class="author-block"><sup>2</sup>Mantra Inc.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2404.13993"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.13993"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/liyingxuan1012/zeroshot-speaker-prediction"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="slide.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slide</span>
                </a>
              </span>
              <span class="link-block">
                <a href="poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png"/>
      <h2 class="subtitle has-text-centered">
        Our framework can predict character labels of unseen comics only from images.<br>
        Courtesy of Kiriga Yuki.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recognizing characters and predicting speakers of dialogue are critical for comic processing tasks, such as voice generation or translation. However, because characters vary by comic title, supervised learning approaches like training character classifiers which require specific annotations for each comic title are infeasible. This motivates us to propose a novel zero-shot approach, allowing machines to identify characters and predict speaker names based solely on unannotated comic images. In spite of their importance in real-world applications, these task have largely remained unexplored due to challenges in story comprehension and multimodal integration. Recent large language models (LLMs) have shown great capability for text understanding and reasoning, while their application to multimodal content analysis is still an open problem. To address this problem, we propose an iterative multimodal framework, the first to employ multimodal information for both character identification and speaker prediction tasks. Our experiments demonstrate the effectiveness of the proposed framework, establishing a robust baseline for these tasks. Furthermore, since our method requires no training data or annotations, it can be used as-is on any comic series.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

        <!-- Framework. -->
        <h2 class="title is-3">Iterative Multimodal Fusion Method</h2>
        <div class="content has-text-justified">
          <p>
            We pioneer in revealing the potential of large language models (LLMs) for comics analysis and propose a novel method that integrates text and image modalities. 
            To address the challenges of integrating LLMs with other modules and enhance the machine's comprehension of comics, we introduce an iterative framework. 
            We merge text-based LLM predictions with image-based classifiers, and alternately refine each module using results from the other. 
          </p>
        </div>
        <figure class="content has-text-centered">
          <img src="./static/images/pipeline.png"/>
          <figcaption>Our proposed framework for zero-shot character identification and speaker prediction in comics.<br>
          Courtesy of Ito Shinpei.</figcaption>
        </figure>
        <!--/ Framework. -->


        <!-- Main result. -->
        <h2 class="title is-3">Quantitative results</h2>
        <div class="content has-text-justified">
          <p>
            In the main evaluation, we omit the steps of object detection, OCR, and character name extraction in data preprocessing. That is, we simplify the tasks of speaker prediction and character identification to classify the character labels for the character and text regions.
            The experimental results suggest that our iterative process is effective, particularly when the prediction of relationships between text and character regions is accurate.
          </p>
        </div>
        <figure class="content has-text-centered">
          <img src="./static/images/result.png"/>
          <figcaption>Speaker prediction and character identification accuracy (%).</figcaption>
        </figure>

        <h2 class="title is-3">Qualitative results</h2>
        <div class="content has-text-justified">
          <p>
            To validate the effectiveness of our proposed iterative multimodal fusion method, we conducted evaluations in two aspects: <b>Unimodal vs. Multimodal</b> and <b>One-Step vs. Iterative</b>.
          </p>
        </div>
        <h3 class="title is-4">Unimodal vs. Multimodal</h3>
        <figure class="content has-text-centered">
          <img src="./static/images/case1.png"/>
          <figcaption><b>Speaker prediction results obtained using a single modality (textual information) and multiple modalities combined. Courtesy of Tashiro Kimu, Hikochi Sakuya, Yoshimori Mikio, Karikawa Seyu.</b></figcaption>
          <img src="./static/images/case2.png"/>
          <figcaption>Speaker prediction results obtained using a single modality (visual information) and multiple modalities combined. * indicates that the image-based method used the ground truth of character labels.<br>
          Courtesy of Ayumi Yui, Hanada Sakumi.</figcaption>
        </figure>
        <h3 class="title is-4">One-Step vs. Iterative</h3>
        <figure class="content has-text-centered">
          <img src="./static/images/case3_1.png"/>
          <img src="./static/images/case3_2.png"/>
          <figcaption>Results of character identification and speaker prediction across different iterations. (Accuracy of speaker pred. & character id.). Courtesy of Tenya, Saki Kaori, Ayumi Yui, Karikawa Seyu, Matsuda Naomasa.</figcaption>
        </figure>
        <!--/ Main result. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{li2024zeroshot,
        title={Zero-shot character identification and speaker prediction in comics via iterative multimodal fusion},
        author={Li, Yingxuan and Hinami, Ryota and Aizawa, Kiyoharu and Matsui, Yusuke},
        booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
        year={2024}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We thank the authors of <a href="https://github.com/nerfies/nerfies.github.io" class="external-link">Nerfies</a> that kindly open sourced the template of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
